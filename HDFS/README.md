## Домашнее задание по HDFS

В этом домашнем задании Вам предстоит поближе познакомиться с [распределенной файловой системой HDFS](http://hadoop.apache.org/docs/r1.2.1/hdfs_design.html). Ваша цель - написать скрипты, которые будут делать HTTP запросы к различным демонам HDFS или вызывать стандартные утилиты командной строки (shell) с целью получения запрашиваемой информации.

Можно использовать любой скриптовый язык программирования. Самый простой вариант - использовать bash, а также утилиты curl, grep, wc и т.п.

### Задачи

#### 01
На вход скрипту подается имя файла, на выходе нужно получить имя сервера или IP-адрес, с которого будет читаться первый блок данных (реплик может быть несколько, засчитываться будет любой из них). Пример:

	$ ./run.sh /data/access_logs/big_log/access.log.2015-12-10
	mipt-node01.atp-fivt.org

#### 02
На вход скрипту подается имя файла, на выходе нужно получить первые 10 байт этого файла (hadoop fs -cat / head, tail и hdfs dfs -cat.. использовать нельзя)

	$ ./run.sh /data/access_logs/big_log/access.log.2015-12-10
	41.190.60.

#### 03

На вход скрипту подается идентификатор блока, на выходе нужно получить имя сервера (если их несколько, то выбрать любой), где хранится данный блок и физический путь в локальной файловой системе до этого блока данных. (О том, как зайти на ноды кластера, написано в материалам [cеминара по HDFS](/distribute/practice/01-hdfs.md))

	$ ./run.sh blk_1075127191
	bds03.vdi.mipt.ru:/dfs/dn/current/BP-76251478-10.55.163.141-1427134131440/current/finalized/subdir21/subdir35/blk_1075127191
